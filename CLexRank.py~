'''
Created on May 4, 2015

@author: rmn
'''
from __future__ import division
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer
import codecs
import json
import networkx as nx
import community
import matplotlib.pyplot as plt
from _collections import defaultdict
from plaintext import PlaintextParser
from tokenizers import Tokenizer
from lex_rank import LexRankSummarizer
from sumy.nlp.stemmers import Stemmer
from summarizer.base import Summarizer
from util.tokenization import WordTokenizer


class CLexRank(Summarizer):
    '''
    Implementation of CLexRank Algorithm

    Qazvinian, V., Radev, D. R., Mohammad, S., Dorr, B. J., Zajic, D. M., Whidby,
        M., & Moon, T. (2013). Generating Extractive
        Summaries of Scientific Paradigms. J. Artif. Intell.
        Res.(JAIR), 46, 165-201.
    '''

    def __init__(self):
        #         '''
        #         Constructor
        #
        #         Args:
        #             citations(list)
        #                 A list of strings containing citation texts
        #
        #         Important data structures being initialized:
        #             self.G: The graph of citations
        #             self.cits: Graph edges
        #                 A list of tuples: [(node1, node2, similarity), ...]
        #             self.cit_dict: Dictionary of citations
        #                 Mapping each citations text with its id
        #             self.cit_text: Dictionary of citations texts
        #         '''
        pass

    def draw_graph(self):
        '''
        Draws the graph
        '''
        nx.draw(self.G)
        plt.show()

    def _partition(self):
        '''
        Partitions the graph to find the partition that maximizes
            the graph modularity

        Returns
        ------
        dict: Mapping nodes to partitions

        '''
        return community.best_partition(self.G)

    def get_graph(self):
        return self.G

    def summarize(self, citations, max_length=250):
        '''
        Ranks sentences within each cluster to build the final summary
            Using the LexRank algorithm

        Args:
            citations(list)
                A list of strings
            max_length(int)
                maximum length of the summary

        Returns
        -------
        List
            A list of ranked strings for the final summary
        '''
        vectorizer = TfidfVectorizer(
            tokenizer=self.tokenize, min_df=1, max_df=len(citations) * .8)
        cit_vectors = vectorizer.fit_transform(citations).toarray()
        self.cit_text = {i: v for i, v in enumerate(citations)}
        cit_dict = {i: v for i, v in enumerate(cit_vectors)}
        self.cit_dict = cit_dict

        self.cits = []
        for e in cit_dict:  # vector (numpy array)
            for e1 in cit_dict:  # target_vector
                if e != e1:
                    self.cits.append(
                        (e, e1, self.cossim(cit_dict[e], cit_dict[e1])))
#         avg_sim = np.mean([c[2] for c in self.cits])
#         self.cits = [c for c in self.cits if c[2] > avg_sim]
#             cit_network.append(self.cits)

        self.G = nx.Graph()
        self.G.add_weighted_edges_from(self.cits)
        part = self._partition()
        clusters = defaultdict(list)
        for k, v in part.iteritems():
            clusters[v].append(self.cit_text[k])

        # running lexrank on each cluster
        summary = {}
        for i in clusters:
            parser = PlaintextParser.from_string(
                '. '.join(clusters[i]), Tokenizer('english'))
            summarizer = LexRankSummarizer(Stemmer('english'))
            summ = summarizer(parser.document, 5)
            summary[i] = [unicode(sent) for sent in summ]
        final_sum = []
        idx = 0
        tokenizer = WordTokenizer(stem=False, alg='word')
        while tokenizer.count_words(final_sum) < max_length:
            for k in summary:
                if idx < len(summary[k]):
                    final_sum.append(summary[k][idx])
            idx += 1
        prev = ''
        while tokenizer.count_words(final_sum) > max_length:
            prev = final_sum.pop()
#         if prev != '':
#             terms = tokenizer(prev)
#             final_sum.append(prev)
#             final_sum.extend([' '.join(terms[:len(terms) // 2])])
        print tokenizer.count_words(final_sum)
        return final_sum




#     nx.draw_spring(part)
